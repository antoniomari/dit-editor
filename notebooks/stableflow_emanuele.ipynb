{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "parent_dir = Path.cwd().parent.resolve()\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Verify that the path has been added correctly\n",
    "print(sys.path[0])\n",
    "\n",
    "from diffusers import FluxPipeline\n",
    "from diffusers.models import AutoencoderTiny\n",
    "import torch\n",
    "import optparse\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch/nevali'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/nevali'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/scratch/nevali'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a94522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from accelerate.utils import release_memory\n",
    "\n",
    "def clear_all_gpu_memory():\n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Get number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {num_gpus} GPU(s).\")\n",
    "\n",
    "    # Iterate through each GPU\n",
    "    for device_id in range(num_gpus):\n",
    "        with torch.cuda.device(device_id):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.reset_accumulated_memory_stats()\n",
    "            release_memory()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.ipc_collect()\n",
    "    print(\"GPU memory cleared across all available devices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da404459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FluxPipeline\n",
    "\n",
    "dtype = torch.float16\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", \n",
    "                                    device_map=\"balanced\",\n",
    "                                    torch_dtype=dtype)\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cache_and_edit\n",
    "reload(cache_and_edit)\n",
    "from cache_and_edit import *\n",
    "import cache_and_edit.hooks\n",
    "reload(cache_and_edit.hooks)\n",
    "\n",
    "cached_pipe = CachedPipeline(pipe)\n",
    "# cached_pipe_dest = CachedPipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ac7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "def resize_bounding_box(\n",
    "    bb_mask: torch.Tensor,\n",
    "    target_size: Tuple[int, int] = (64, 64),\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a bounding box mask, patches it into a mask with the target size.\n",
    "    The mask is a 2D tensor of shape (H, W) where each element is either 0 or 1.\n",
    "    Any patch that contains at least one 1 in the original mask will be set to 1 in the output mask.\n",
    "\n",
    "    Args:\n",
    "        bb_mask (torch.Tensor): The bounding box mask as a boolean tensor of shape (H, W).\n",
    "        target_size (Tuple[int, int]): The size of the target mask as a tuple (H, W).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The resized bounding box mask as a boolean tensor of shape (H, W).\n",
    "    \"\"\"\n",
    "    \n",
    "    w_mask, h_mask = bb_mask.shape[-2:]\n",
    "    w_target, h_target = target_size\n",
    "\n",
    "    # Make sure the sizes are compatible\n",
    "    if w_mask % w_target != 0 or h_mask % h_target != 0:\n",
    "        raise ValueError(\n",
    "            f\"Mask size {bb_mask.shape[-2:]} is not compatible with target size {target_size}\"\n",
    "        )\n",
    "    \n",
    "    # Compute the size of a patch\n",
    "    patch_size = (w_mask // w_target, h_mask // h_target)\n",
    "    print(f\"patch_size: {patch_size}\")\n",
    "\n",
    "    # Iterate over the mask, one patch at a time, and save a 0 patch if the patch is empty or a 1 patch if the patch is not empty\n",
    "    out_mask = torch.zeros((w_target, h_target), dtype=bb_mask.dtype, device=bb_mask.device)\n",
    "    for i in range(w_target):\n",
    "        for j in range(h_target):\n",
    "            patch = bb_mask[\n",
    "                i * patch_size[0] : (i + 1) * patch_size[0],\n",
    "                j * patch_size[1] : (j + 1) * patch_size[1],\n",
    "            ]\n",
    "            if torch.sum(patch) > 0:\n",
    "                out_mask[i, j] = 1\n",
    "            else:\n",
    "                out_mask[i, j] = 0\n",
    "\n",
    "    return out_mask\n",
    "\n",
    "\n",
    "def get_combined_latents(\n",
    "    bg_latents: torch.Tensor,\n",
    "    fg_latents: torch.Tensor,\n",
    "    bb_mask: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given a background and foreground latents, and a bounding box mask,\n",
    "    combine the latents into a single tensor by putting the foreground latents\n",
    "    inside the bounding box mask and the background latents outside of it.\n",
    "\n",
    "    Args:\n",
    "        bg_latents (torch.Tensor): The background latents as a tensor of shape (H * W, C).\n",
    "        fg_latents (torch.Tensor): The foreground latents as a tensor of shape (H * W, C).\n",
    "        bb_mask (torch.Tensor): The bounding box mask as a boolean tensor of shape (H * W, ).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The combined latents as a tensor of shape (H * W, C).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if bb_mask.ndim == 2:\n",
    "        bb_mask = bb_mask.flatten()\n",
    "\n",
    "    assert bg_latents.shape == fg_latents.shape, \"Background and foreground latents must have the same shape\"\n",
    "    assert bg_latents.shape[0] == bb_mask.shape[0], \"Background latents and bounding box mask must have the same number of elements\"\n",
    "    assert fg_latents.shape[0] == bb_mask.shape[0], \"Foreground latents and bounding box mask must have the same number of elements\"\n",
    "\n",
    "    if bb_mask.dtype == torch.bool:\n",
    "        bb_mask = bb_mask.float()\n",
    "    if bb_mask.ndim == 1:\n",
    "        bb_mask = bb_mask.unsqueeze(1)\n",
    "\n",
    "    \n",
    "    # Create a new tensor to hold the combined latents\n",
    "    combined_latents = bg_latents * (1 - bb_mask) + fg_latents * bb_mask\n",
    "    return combined_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c232353",
   "metadata": {},
   "outputs": [],
   "source": [
    " # [0, 1, 2, 17, 18, 25, 28, 53, 54, 56].  [28, 53, 54, 56, 25]\n",
    "vital_layers = [f\"transformer.transformer_blocks.{i}\" for i in [0, 1, 17, 18]] + \\\n",
    "                [f\"transformer.single_transformer_blocks.{i-19}\" for i in [25, 28, 53, 54, 56]]\n",
    "mask_path = \"../data/Real-Real/0001 a professional photograph of a puppy in the snow, ultra realistic/mask_bg_fg.jpg\"\n",
    "mask = Image.open(mask_path).convert(\"L\")\n",
    "# convert to tensor\n",
    "mask = np.array(mask)\n",
    "mask = torch.from_numpy(mask)\n",
    "mask = mask / 255.0\n",
    "mask = resize_bounding_box(mask, target_size=(64, 64)).flatten().unsqueeze(1)\n",
    "prompts = [\"a house in the forest\", \"a green cow\", \"a house in the forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(imgs: list):\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(len(imgs)*5, 5))\n",
    "    for ax, im in zip(axes, imgs):\n",
    "        ax.imshow(im)\n",
    "        ax.axis('off')  # hide axes ticks\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.benchmark_data import gather_images\n",
    "os_path = \"../data/\"\n",
    "all_images = gather_images(os_path)\n",
    "\n",
    "example = all_images[0]\n",
    "print(example.category)\n",
    "print(example.prompt)\n",
    "\n",
    "# Plot the example\n",
    "example.plot_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5228493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cache_and_edit.inversion import * \n",
    "\n",
    "cut_img = torch.tensor(np.array(example.fg_mask.resize(example.fg_image.size))).to(torch.bool).unsqueeze(-1) * torch.tensor(np.array(example.fg_image))\n",
    "reframed_fg_img, resized_mask = place_image_in_bounding_box(\n",
    "    cut_img,\n",
    "    (torch.from_numpy(np.array(example.target_mask)) / 255.0).to(dtype=bool)\n",
    ")\n",
    "\n",
    "reframed_fg_img = Image.fromarray(reframed_fg_img.numpy())\n",
    "\n",
    "bg_noise = get_inverted_input_noise(cached_pipe, example.bg_image, 100)\n",
    "fg_noise = get_inverted_input_noise(cached_pipe, reframed_fg_img, 100)\n",
    "\n",
    "display(cached_pipe.run(\n",
    "    \"\",\n",
    "    num_inference_steps=28,\n",
    "    seed=42,\n",
    "    guidance_scale=3.5,\n",
    "    latents=fg_noise.unsqueeze(0),\n",
    "    width=reframed_fg_img.size[0],\n",
    "    height=reframed_fg_img.size[1]\n",
    ").images[0].resize((256, 256)))\n",
    "\n",
    "display(cached_pipe.run(\n",
    "    \"\",\n",
    "    num_inference_steps=28,\n",
    "    seed=42,\n",
    "    guidance_scale=3.5,\n",
    "    latents=bg_noise.unsqueeze(0),\n",
    "    width=example.bg_image.size[0],\n",
    "    height=example.bg_image.size[1]\n",
    ").images[0].resize((256, 256)))\n",
    "\n",
    "resized_mask = resize_bounding_box(resized_mask, (32, 32)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = cached_pipe.run_inject_qkv(\n",
    "    [\"\", \"\", \"\"],\n",
    "    num_inference_steps=28,\n",
    "    seed=42,\n",
    "    guidance_scale=0,\n",
    "    empty_clip_embeddings=False,\n",
    "    q_mask=resized_mask.unsqueeze(1),\n",
    "    positions_to_inject = [f\"transformer.transformer_blocks.{i}\" for i in range(19)]  +  [f\"transformer.single_transformer_blocks.{i}\" for i in range(38)],\n",
    "    positions_to_inject_foreground = [f\"transformer.transformer_blocks.{i}\" for i in range(19)] + [f\"transformer.single_transformer_blocks.{i}\" for i in range(38)], #  + [f\"transformer.single_transformer_blocks.{i}\" for i in range(2)],\n",
    "    latents=torch.stack([bg_noise, fg_noise, bg_noise]),\n",
    "    width=512,\n",
    "    height=512\n",
    ").images\n",
    "\n",
    "display_images(images)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
