{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f70ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nevali/cs503_rl/project/dit-ablations/notebooks\n",
      "/home/nevali/cs503_rl/project/dit-ablations/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "print(os.getcwd())\n",
    "print(os.getcwd())\n",
    "\n",
    "parent_dir = Path.cwd().parent.resolve()\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bb5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import gc\n",
    "from accelerate.utils import release_memory\n",
    "\n",
    "from data.benchmark_data import gather_images\n",
    "from cache_and_edit.inversion import place_image_in_bounding_box, get_inverted_input_noise, resize_bounding_box, compose_noise_masks\n",
    "\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import cache_and_edit\n",
    "reload(cache_and_edit)\n",
    "from cache_and_edit import *\n",
    "import cache_and_edit.hooks\n",
    "reload(cache_and_edit.hooks)\n",
    "\n",
    "\n",
    "# os.environ['HF_HOME'] = '/dlabscratch1/anmari'\n",
    "# os.environ['TRANSFORMERS_CACHE'] = '/dlabscratch1/anmari'\n",
    "# os.environ['HF_DATASETS_CACHE'] = '/dlabscratch1/anmari'\n",
    "\n",
    "\n",
    "os.environ['HF_HOME'] = '/scratch/nevali'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/nevali'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/scratch/nevali'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc31840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def clear_all_gpu_memory():\n",
    "    # Run garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    # Get number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {num_gpus} GPU(s).\")\n",
    "\n",
    "    # Iterate through each GPU\n",
    "    for device_id in range(num_gpus):\n",
    "        with torch.cuda.device(device_id):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.reset_accumulated_memory_stats()\n",
    "            release_memory()\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.ipc_collect()\n",
    "    print(\"GPU memory cleared across all available devices.\")\n",
    "\n",
    "def display_side_by_side(image1, image2, image3, titles=None):\n",
    "    \"\"\"\n",
    "    Display three images side by side with optional titles\n",
    "    \n",
    "    Args:\n",
    "        image1, image2, image3: PIL Images to display\n",
    "        titles: List of titles for each image. If None, no titles are shown.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Display each image\n",
    "    axes[0].imshow(image1)\n",
    "    axes[1].imshow(image2)\n",
    "    axes[2].imshow(image3)\n",
    "    \n",
    "    # Set titles if provided\n",
    "    if titles is not None:\n",
    "        for i, title in enumerate(titles):\n",
    "            axes[i].set_title(title)\n",
    "    \n",
    "    # Remove axis ticks\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "clear_all_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78fb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cache_and_edit.flux_pipeline import EditedFluxPipeline\n",
    "\n",
    "dtype = torch.float16\n",
    "pipe = EditedFluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", \n",
    "                                    device_map=\"balanced\",\n",
    "                                    torch_dtype=dtype)\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "# Load cached pipeline\n",
    "cached_pipe = CachedPipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4379d388",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'benchmark_images_generations/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m os_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmark_images_generations/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m all_images \u001b[38;5;241m=\u001b[39m \u001b[43mgather_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m example \u001b[38;5;241m=\u001b[39m all_images[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(example\u001b[38;5;241m.\u001b[39mcategory)\n",
      "File \u001b[0;32m~/cs503_rl/project/dit-ablations/data/benchmark_data.py:179\u001b[0m, in \u001b[0;36mgather_images\u001b[0;34m(image_dir)\u001b[0m\n\u001b[1;32m    176\u001b[0m missing_image_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Get all directories in the image_dir\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    180\u001b[0m     category_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, category)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(category_path): \u001b[38;5;66;03m# skip .DS_Store\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'benchmark_images_generations/'"
     ]
    }
   ],
   "source": [
    "os_path = \"benchmark_images_generations/\"\n",
    "all_images = gather_images(os_path)\n",
    "\n",
    "example = all_images[0]\n",
    "print(example.category)\n",
    "print(example.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55660f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cache_and_edit.inversion import compose_noise_masks\n",
    "example = all_images[76]\n",
    "example.plot_sample()\n",
    "example_noise = compose_noise_masks(cached_pipe, \n",
    "              example.fg_image, \n",
    "              example.bg_image, \n",
    "              example.target_mask, \n",
    "              example.fg_mask, \n",
    "              option=\"segmentation1\", \n",
    "              num_inversion_steps=50,\n",
    "              photoshop_fg_noise=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665378e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from cache_and_edit.qkv_cache import TFICONAttnProcessor\n",
    "\n",
    "NUM_STEPS = 50\n",
    "print(\"Available Masks:\")\n",
    "for key in example_noise[\"latent_masks\"].keys():\n",
    "    print(key)\n",
    "print(\"Available Noises:\")\n",
    "for key in example_noise[\"noise\"].keys():\n",
    "    print(key)\n",
    "\n",
    "# alpha_random = 0.2 nicely working\n",
    "alpha_random = 0.05\n",
    "\n",
    "\n",
    "vital_layers = [f\"transformer.transformer_blocks.{i}\" for i in [0, 1, 17, 18]] + \\\n",
    "                [f\"transformer.single_transformer_blocks.{i-19}\" for i in [25, 28, 53, 54, 56]]\n",
    "\n",
    "all_layers = [f\"transformer.transformer_blocks.{i}\" for i in range(19)] + \\\n",
    "                [f\"transformer.single_transformer_blocks.{i-19}\" for i in range(19, 57)]\n",
    "\n",
    "\n",
    "# use vital layers on foreground to adhere at prompt\n",
    "# use foreground noise inside mask otherwise it doesnt work or the result is blurry\n",
    "# try [\"A cartoon forest\", \"A goat\", \"A green goat in a cartoon forest with a hat\"],\n",
    "\n",
    "images = cached_pipe.run_inject_qkv(\n",
    "    [\"\", \"\", example.prompt],\n",
    "    # [\"\", \"\", \"A goat\"],\n",
    "    num_inference_steps=NUM_STEPS,\n",
    "    seed=42,\n",
    "    guidance_scale=3,\n",
    "    positions_to_inject=all_layers,\n",
    "    positions_to_inject_foreground=vital_layers,\n",
    "    empty_clip_embeddings=False,\n",
    "    q_mask=example_noise[\"latent_masks\"][\"latent_segmentation_mask\"],\n",
    "    latents=torch.stack(\n",
    "                        [\n",
    "                        example_noise[\"noise\"][\"background_noise\"],\n",
    "                        example_noise[\"noise\"][\"foreground_noise\"],\n",
    "                        torch.where(\n",
    "                            example_noise[\"latent_masks\"][\"latent_segmentation_mask\"] > 0,\n",
    "                            alpha_random * torch.randn_like(example_noise[\"noise\"][\"foreground_noise\"]) + (1 - alpha_random) * example_noise[\"noise\"][\"foreground_noise\"],\n",
    "                            example_noise[\"noise\"][\"background_noise\"],\n",
    "                        ),\n",
    "                        ]\n",
    "                    ),\n",
    "    # latents=torch.stack([bg_noise, fg_noise, tficon_noise]),\n",
    "    processor_class=partial(\n",
    "        TFICONAttnProcessor,\n",
    "        call_max_times=int(0.4 * NUM_STEPS),\n",
    "        inject_q=True,\n",
    "        inject_k=True,\n",
    "        inject_v=True,\n",
    "        ),\n",
    "    width=512,\n",
    "    height=512,\n",
    "    inverted_latents_list = list(zip(example_noise[\"noise\"][\"background_noise_list\"], example_noise[\"noise\"][\"foreground_noise_list\"])),\n",
    "    tau_b=0.8,\n",
    "    bg_consistency_mask=example_noise[\"latent_masks\"][\"latent_segmentation_mask\"],\n",
    ")\n",
    "\n",
    "display_side_by_side(images[0][0], images[0][1],images[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc8324",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from evaluation.eval import get_scores_for_single_example\n",
    "\n",
    "methods = [\"naive\", \"tf-icon\", \"kv-edit\", \"ours\"]\n",
    "\n",
    "get_scores_for_single_example(example, methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7e484",
   "metadata": {},
   "source": [
    "### NOTE: for each ablation report the results for each of the 4 domains in the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48354aa3",
   "metadata": {},
   "source": [
    "### Ablation on queries keys and values (remove eachone once at a time), report all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08886397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d4c728f",
   "metadata": {},
   "source": [
    "### Ablation on alpha noise (check metric to compute similarity to original image (naive-baseline or only foreground)). How much noise can you add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82d1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f225a3",
   "metadata": {},
   "source": [
    "### Ablations on the timesteps:\n",
    "1. fixed tau_alpha and tau_beta --> vary the number of steps for inversion and denoising.\n",
    "    - hestetic and bg_consistency vs gflops\n",
    "2. fixed the number of steps and tau_alpha --> vary tau_beta\n",
    "    - if tau_beta varies we expect a trade-off between hestetic and bg_consist.\n",
    "3. fixed the number of steps and tau_beta --> vary tau_alpha\n",
    "    - clip_text_image and dinov2_similarity and hestetic_score (we might expect that the longer you inject the more similarity you get but less hestetic score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337eb8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a611ae0",
   "metadata": {},
   "source": [
    "### Classifier free guidance coefficient (check different values now we are using 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e017c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95c725de",
   "metadata": {},
   "source": [
    "### Effect of the prompt on the output image (for now let's check these two cases):\n",
    "1. No target prompt\n",
    "2. Full prompt (folder name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea42e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KV-Edit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
